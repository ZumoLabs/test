{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"zpy : The Synthetic Data Toolkit Collecting, labeling, and cleaning data for computer vision is a pain. Jump into the future and create your own data instead! Synthetic data is faster to develop with, effectively infinite, and gives you full control to prevent bias and privacy issues from creeping in. We created zpy to make synthetic data easy, by simplifying the simulation (sim) creation process and providing an easy way to generate synthetic data at scale.","title":"About"},{"location":"#zpy-the-synthetic-data-toolkit","text":"Collecting, labeling, and cleaning data for computer vision is a pain. Jump into the future and create your own data instead! Synthetic data is faster to develop with, effectively infinite, and gives you full control to prevent bias and privacy issues from creeping in. We created zpy to make synthetic data easy, by simplifying the simulation (sim) creation process and providing an easy way to generate synthetic data at scale.","title":"zpy: The Synthetic Data Toolkit"},{"location":"addon/about/","text":"Blender Addon zpy comes with a Blender Addon to make it easier to design and create sims within Blender. You can learn more about Blender Add-Ons here . Features Debug Locally: Run simulations locally for debugging. Render a single frame from a sim . Segmenting : Individual objects or groups of objects as single instances or categories. Import categories from .txt or .json . Scripting: Get started quick with script templates . Exporting: Export sims for cloud usage through WebApp or CLI .","title":"About"},{"location":"addon/about/#blender-addon","text":"zpy comes with a Blender Addon to make it easier to design and create sims within Blender. You can learn more about Blender Add-Ons here .","title":"Blender Addon"},{"location":"addon/about/#features","text":"Debug Locally: Run simulations locally for debugging. Render a single frame from a sim . Segmenting : Individual objects or groups of objects as single instances or categories. Import categories from .txt or .json . Scripting: Get started quick with script templates . Exporting: Export sims for cloud usage through WebApp or CLI .","title":"Features"},{"location":"addon/install/","text":"Install the Blender Addon Once you have installed the zpy module into Blender's python , download the latest zip (you want the one called zpy_addon-v*.zip ). Then open up Blender. Navigate to Edit -> Preferences -> Add-ons . You should be able to install and enable the addon from there. Video Tutorial You can watch this tutorial as a video on YouTube:","title":"Install"},{"location":"addon/install/#install-the-blender-addon","text":"Once you have installed the zpy module into Blender's python , download the latest zip (you want the one called zpy_addon-v*.zip ). Then open up Blender. Navigate to Edit -> Preferences -> Add-ons . You should be able to install and enable the addon from there.","title":"Install the Blender Addon"},{"location":"addon/install/#video-tutorial","text":"You can watch this tutorial as a video on YouTube:","title":"Video Tutorial"},{"location":"app/about/","text":"WebApp The web app is available at app.zumolabs.ai . The app is used to visualize dataset, sims , and jobs on the backend and get insights on why your synthetic data is not giving you the desired performance. Features Download datasets Visualize datasets View metadata for sims and datasets Mange teams and data permissions","title":"About"},{"location":"app/about/#webapp","text":"The web app is available at app.zumolabs.ai . The app is used to visualize dataset, sims , and jobs on the backend and get insights on why your synthetic data is not giving you the desired performance.","title":"WebApp"},{"location":"app/about/#features","text":"Download datasets Visualize datasets View metadata for sims and datasets Mange teams and data permissions","title":"Features"},{"location":"app/create_an_account/","text":"Create an Account WebApp accounts are required in order to use the ZumoLabs cloud generation. To get started: Head to app.zumolabs.ai . Sign in with Google, or create an account using your preferred email address. Verify your email address. Start generating data and sims!","title":"Create an Account"},{"location":"app/create_an_account/#create-an-account","text":"WebApp accounts are required in order to use the ZumoLabs cloud generation. To get started: Head to app.zumolabs.ai . Sign in with Google, or create an account using your preferred email address. Verify your email address. Start generating data and sims!","title":"Create an Account"},{"location":"cli/about/","text":"zpy cli The zpy cli is a C ommand L ine I nterface to interact with datasets and sims on the zumo labs backend. This CLI has much of the same functionality found in the WebApp , but allows developers to use command line instead of a GUI to interact with datasets and sims. Usage zpy config - Authenticate with backend. zpy project - Swap between projects. zpy dataset - Upload or generate datasets. zpy sim - sims zpy job - Run jobs on datasets to product output datasets. zpy transform - Format datasets.","title":"About"},{"location":"cli/about/#zpy-cli","text":"The zpy cli is a C ommand L ine I nterface to interact with datasets and sims on the zumo labs backend. This CLI has much of the same functionality found in the WebApp , but allows developers to use command line instead of a GUI to interact with datasets and sims.","title":"zpy cli"},{"location":"cli/about/#usage","text":"zpy config - Authenticate with backend. zpy project - Swap between projects. zpy dataset - Upload or generate datasets. zpy sim - sims zpy job - Run jobs on datasets to product output datasets. zpy transform - Format datasets.","title":"Usage"},{"location":"cli/basic/","text":"Authenticate with the backend zpy login Verify CLI configuration zpy config Display help zpy help Display zpy version zpy version","title":"zpy config"},{"location":"cli/basic/#authenticate-with-the-backend","text":"zpy login","title":"Authenticate with the backend"},{"location":"cli/basic/#verify-cli-configuration","text":"zpy config","title":"Verify CLI configuration"},{"location":"cli/basic/#display-help","text":"zpy help","title":"Display help"},{"location":"cli/basic/#display-zpy-version","text":"zpy version","title":"Display zpy version"},{"location":"cli/dataset/","text":"List datasets zpy dataset list Download dataset zip zpy dataset get <name> path/to/local Upload dataset zip zpy dataset upload <name> path/to/dataset.zip Generate images for dataset from sim zpy dataset generate <name> <sim_name> <num_frames> args..","title":"zpy dataset"},{"location":"cli/dataset/#list-datasets","text":"zpy dataset list","title":"List datasets"},{"location":"cli/dataset/#download-dataset-zip","text":"zpy dataset get <name> path/to/local","title":"Download dataset zip"},{"location":"cli/dataset/#upload-dataset-zip","text":"zpy dataset upload <name> path/to/dataset.zip","title":"Upload dataset zip"},{"location":"cli/dataset/#generate-images-for-dataset-from-sim","text":"zpy dataset generate <name> <sim_name> <num_frames> args..","title":"Generate images for dataset from sim"},{"location":"cli/env/","text":"Add a backend environment to target zpy env add <name> <endpoint> Swap to backend environment zpy env set <name>","title":"zpy env"},{"location":"cli/env/#add-a-backend-environment-to-target","text":"zpy env add <name> <endpoint>","title":"Add a backend environment to target"},{"location":"cli/env/#swap-to-backend-environment","text":"zpy env set <name>","title":"Swap to backend environment"},{"location":"cli/job/","text":"List jobs zpy job list Create job zpy job create <name> <operation> -f <dataset_filter> Download logs from job zpy job logs <name> path/to/local","title":"zpy job"},{"location":"cli/job/#list-jobs","text":"zpy job list","title":"List jobs"},{"location":"cli/job/#create-job","text":"zpy job create <name> <operation> -f <dataset_filter>","title":"Create job"},{"location":"cli/job/#download-logs-from-job","text":"zpy job logs <name> path/to/local","title":"Download logs from job"},{"location":"cli/project/","text":"List accounts zpy account list List projects zpy project list Set current project zpy project set <uuid> Create new project zpy project create <account_uuid> <name> Clear current project zpy project clear","title":"zpy project"},{"location":"cli/project/#list-accounts","text":"zpy account list","title":"List accounts"},{"location":"cli/project/#list-projects","text":"zpy project list","title":"List projects"},{"location":"cli/project/#set-current-project","text":"zpy project set <uuid>","title":"Set current project"},{"location":"cli/project/#create-new-project","text":"zpy project create <account_uuid> <name>","title":"Create new project"},{"location":"cli/project/#clear-current-project","text":"zpy project clear","title":"Clear current project"},{"location":"cli/sim/","text":"List sims zpy sim list Download sim zip zpy sim get <name> path/to/local Upload sim zip zpy sim upload <name> path/to/sim.zip Download logs from sim ingress zpy sim logs <name> path/to/local","title":"zpy sim"},{"location":"cli/sim/#list-sims","text":"zpy sim list","title":"List sims"},{"location":"cli/sim/#download-sim-zip","text":"zpy sim get <name> path/to/local","title":"Download sim zip"},{"location":"cli/sim/#upload-sim-zip","text":"zpy sim upload <name> path/to/sim.zip","title":"Upload sim zip"},{"location":"cli/sim/#download-logs-from-sim-ingress","text":"zpy sim logs <name> path/to/local","title":"Download logs from sim ingress"},{"location":"cli/transform/","text":"List transforms zpy transform list Run transform on a dataset zpy transform dataset <dataset_name> <operation>","title":"zpy transform"},{"location":"cli/transform/#list-transforms","text":"zpy transform list","title":"List transforms"},{"location":"cli/transform/#run-transform-on-a-dataset","text":"zpy transform dataset <dataset_name> <operation>","title":"Run transform on a dataset"},{"location":"overview/citation/","text":"Citation If you use zpy in your research, we would appreciate the citation! @article { zpy , title = {zpy: Synthetic data for Blender.} , author = {Ponte, H. and Ponte, N. and Crowder, S.} , journal = {GitHub. Note: https://github.com/ZumoLabs/zpy} , volume = {1} , year = {2021} }","title":"Citation"},{"location":"overview/citation/#citation","text":"If you use zpy in your research, we would appreciate the citation! @article { zpy , title = {zpy: Synthetic data for Blender.} , author = {Ponte, H. and Ponte, N. and Crowder, S.} , journal = {GitHub. Note: https://github.com/ZumoLabs/zpy} , volume = {1} , year = {2021} }","title":"Citation"},{"location":"overview/code_of_conduct/","text":"Code of Conduct Zumo Labs is committed to providing a friendly, safe and welcoming environment for all, regardless of level of experience, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, nationality, or other traits or characteristics. We ask that you contribute to maintaining a positive environment. We do not tolerate harassment of participants in any form. Participants asked to stop any harassing behavior are expected to comply immediately. Examples of behavior that contributes to creating a positive environment include: Being kind and courteous to others Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Collaborating with other community members Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and sexual attention or advances The use of inappropriate images, including in a community member's avatar The use of inappropriate language, including in a community member's nickname Any spamming, flaming, baiting, or other attention-stealing behavior Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Discussing topics that are overly polarizing, sensitive, or incite arguments. This includes the discussion of polarizing political views, violence, suicide, and rape. Responding with \u201cRTFM\u201d, \"just google it\u201d or similar phrases in response to help requests Other conduct which could reasonably be considered inappropriate Our Goal The goal of this document is to set the overall tone for our community. This isn\u2019t an exhaustive list of things you can and can't do. Rather, take this document in the spirit in which it\u2019s intended, and try to be your best self. We value many things beyond technical expertise, including collaboration and supporting others within our community. Providing a positive experience for other community members can have a much more significant impact than simply providing the correct answer. Scope This Code of Conduct applies to all spaces managed by Zumo Labs. This includes, but is not limited to, the Discord server, our repositories on GitHub, the YouTube-channel, and meet-ups. In addition, violations of this code outside these spaces may affect a person's ability to participate within them. The ZPY Code of Conduct applies equally to all members of the community, including staff. Attribution This Code of Conduct is adapted from the Python Discord Code of Conduct under a Creative Commons license.","title":"Code of Conduct"},{"location":"overview/code_of_conduct/#code-of-conduct","text":"Zumo Labs is committed to providing a friendly, safe and welcoming environment for all, regardless of level of experience, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, nationality, or other traits or characteristics. We ask that you contribute to maintaining a positive environment. We do not tolerate harassment of participants in any form. Participants asked to stop any harassing behavior are expected to comply immediately. Examples of behavior that contributes to creating a positive environment include: Being kind and courteous to others Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Collaborating with other community members Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and sexual attention or advances The use of inappropriate images, including in a community member's avatar The use of inappropriate language, including in a community member's nickname Any spamming, flaming, baiting, or other attention-stealing behavior Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Discussing topics that are overly polarizing, sensitive, or incite arguments. This includes the discussion of polarizing political views, violence, suicide, and rape. Responding with \u201cRTFM\u201d, \"just google it\u201d or similar phrases in response to help requests Other conduct which could reasonably be considered inappropriate","title":"Code of Conduct"},{"location":"overview/code_of_conduct/#our-goal","text":"The goal of this document is to set the overall tone for our community. This isn\u2019t an exhaustive list of things you can and can't do. Rather, take this document in the spirit in which it\u2019s intended, and try to be your best self. We value many things beyond technical expertise, including collaboration and supporting others within our community. Providing a positive experience for other community members can have a much more significant impact than simply providing the correct answer.","title":"Our Goal"},{"location":"overview/code_of_conduct/#scope","text":"This Code of Conduct applies to all spaces managed by Zumo Labs. This includes, but is not limited to, the Discord server, our repositories on GitHub, the YouTube-channel, and meet-ups. In addition, violations of this code outside these spaces may affect a person's ability to participate within them. The ZPY Code of Conduct applies equally to all members of the community, including staff.","title":"Scope"},{"location":"overview/code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Python Discord Code of Conduct under a Creative Commons license.","title":"Attribution"},{"location":"overview/contribute/","text":"Contribute We welcome community contributions! Our goal is to democratize data creation by building a community of developers that make zpy better everyday. Search through the current issues or open your own. Projects We use GitHub Projects to organize our tickets into boards: Main : Roadmap for the main python package component of zpy. WebApp : Roadmap for the WebApp component of zpy at app.zumolabs.ai API & CLI : Roadmap for the API & CLI component of zpy. ML Experiments : Roadmap for ML Experiments using zpy. Blender Addon : Roadmap for the Blender Addon component of zpy.","title":"Contribute"},{"location":"overview/contribute/#contribute","text":"We welcome community contributions! Our goal is to democratize data creation by building a community of developers that make zpy better everyday. Search through the current issues or open your own.","title":"Contribute"},{"location":"overview/contribute/#projects","text":"We use GitHub Projects to organize our tickets into boards: Main : Roadmap for the main python package component of zpy. WebApp : Roadmap for the WebApp component of zpy at app.zumolabs.ai API & CLI : Roadmap for the API & CLI component of zpy. ML Experiments : Roadmap for ML Experiments using zpy. Blender Addon : Roadmap for the Blender Addon component of zpy.","title":"Projects"},{"location":"overview/domain_randomization/","text":"Domain randomization is a popular technique when generating synthetic data. The key concept behind domain randomization is increasing the variance of certain data parameters in the training set beyond what is seen in the test set. These data parameters may include lighting, camera viewpoint, and asset materials. Models trained on a domain randomized synthetic dataset are more general and suffer less from the sim2real gap.","title":"Domain Randomization"},{"location":"overview/license/","text":"License This release of zpy is under the GPLv3 license, the same license used by Blender . TLDR: Its free, use it!","title":"License"},{"location":"overview/license/#license","text":"This release of zpy is under the GPLv3 license, the same license used by Blender . TLDR: Its free, use it!","title":"License"},{"location":"overview/literature/","text":"Synthetic Data Literature Many papers have been written about synthetic data over the years. If academic papers aren't your jam, we publish articles to explain synthetic data as simply as we can. Below are some key papers organized by ... ... usecase: Robotics: 1 , 3 , 4 , Autonomous Vehicles: 5 , 8 , 9 , 13 , Humans: 2 , 7 , Climate: 11 , ML Theory: 6 , Overview: 10 , Frameworks: 12 , ... year: 2016: 13 , 2017: 2 , 3 , 6 , 2018: 7 , 2019: 1 , 4 , 8 , 10 , 2020: 5 , 9 , 11 , 2021: 12 , TIP The abstracts are also included with the paper links, so a good way to use this document is to ctrl-F the key words relevant to your usecase. Papers Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks Usecase Robotic Grasping Year 2019 Abstract Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to realworld ones. Using domain adaptation methods to cross this \u201creality gap\u201d requires a large amount of unlabelled realworld data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomizedto-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no realworld data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99%. Learning from Simulated and Unsupervised Images through Adversarial Training Usecase Human Gaze Estimation Year 2017 Abstract With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator\u2019s output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a \u2018self-regularization\u2019 term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World Usecase Robotic Grasping Year 2017 Abstract Bridging the \u2018reality gap\u2019 that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control. Deep Drone Racing: From Simulation to Reality with Domain Randomization Usecase Drone Racing Year 2019 Abstract Dynamically changing environments, unreliable state estimation, and operation under severe resource constraints are fundamental challenges that limit the deployment of small autonomous drones. We address these challenges in the context of autonomous, vision-based drone racing in dynamic environments. A racing drone must traverse a track with possibly moving gates at high speed. We enable this functionality by combining the performance of a state-of-the-art planning and control system with the perceptual awareness of a convolutional neural network (CNN). The resulting modular system is both platform- and domain-independent: it is trained in simulation and deployed on a physical quadrotor without any fine-tuning. The abundance of simulated data, generated via domain randomization, makes our system robust to changes of illumination and gate appearance. To the best of our knowledge, our approach is the first to demonstrate zero-shot sim-to-real transfer on the task of agile drone flight. We extensively test the precision and robustness of our system, both in simulation and on a physical platform, and show significant improvements over the state of the art. Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data Usecase Autonomous Vehicles Year 2020 Abstract We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDRgenerated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone. Using Synthetic Data to Train Neural Networks is Model-Based Reasoning Usecase ML Theory Year 2017 Abstract We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data. Learning from Synthetic Humans Usecase Human Pose Detection Year 2018 Abstract Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data. Multi Modal Semantic Segmentation using Synthetic Data Usecase Autonomous Vehicles Year 2019 Abstract Semantic understanding of scenes in threedimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true distance of separation to objects which are crucial for taking mission critical decisions. As a first step, in this work we investigate the possibility of semantically classifying different parts of a given scene in 3D by learning the underlying geometric context in addition to the texture cues BUT in the absence of labelled real-world datasets. To this end we generate a large number of synthetic scenes, their pixel-wise labels and corresponding 3D representations using CARLA software framework. We then build a deep neural network that learns underlying category specific 3D representation and texture cues from color information of the rendered synthetic scenes. Further on we apply the learned model on different real world datasets to evaluate its performance. Our preliminary investigation of results show that the neural network is able to learn the geometric context from synthetic scenes and effectively apply this knowledge to classify each point of a 3D representation of a scene in real-world. Semantic Understanding of Foggy Scenes with Purely Synthetic Data Usecase Autonomous Vehicles Year 2020 Abstract This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen realworld foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data. Synthetic Data for Deep Learning Usecase Overview Year 2019 Abstract Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including syntheticto-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies. Learning color space adaptation from synthetic to real images of cirrus clouds Usecase Cloud Detection Year 2020 Abstract Cloud segmentation plays a crucial role in image analysis for climate modeling. Manually labeling the training data for cloud segmentation is time-consuming and error-prone. We explore to train segmentation networks with synthetic data due to the natural acquisition of pixel-level labels. Nevertheless, the domain gap between synthetic and real images significantly degrades the performance of the trained model. We propose a color space adaptation method to bridge the gap, by training a color-sensitive generator and discriminator to adapt synthetic data to real images in color space. Instead of transforming images by general convolutional kernels, we adopt a set of closed-form operations to make color-space adjustments while preserving the labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and demonstrate the adaptation efficacy on the semantic segmentation task of cirrus clouds. With our adapted synthetic data for training the semantic segmentation, we achieve an improvement of 6:59% when applied to real images, superior to alternative methods. UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments Usecase Framework Year 2021 Abstract Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a Python API for interacting with the virtual environment from Deep Learning frameworks. The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes Usecase Autonomous Vehicles Year 2016 Abstract Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation \u2013 in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task. Title Usecase Year Abstract Title Usecase Year Abstract Title Usecase Year Abstract Title Usecase Year Abstract Title Usecase Year Abstract Title Usecase Year Abstract","title":"Literature"},{"location":"overview/literature/#synthetic-data-literature","text":"Many papers have been written about synthetic data over the years. If academic papers aren't your jam, we publish articles to explain synthetic data as simply as we can. Below are some key papers organized by ... ... usecase: Robotics: 1 , 3 , 4 , Autonomous Vehicles: 5 , 8 , 9 , 13 , Humans: 2 , 7 , Climate: 11 , ML Theory: 6 , Overview: 10 , Frameworks: 12 , ... year: 2016: 13 , 2017: 2 , 3 , 6 , 2018: 7 , 2019: 1 , 4 , 8 , 10 , 2020: 5 , 9 , 11 , 2021: 12 , TIP The abstracts are also included with the paper links, so a good way to use this document is to ctrl-F the key words relevant to your usecase.","title":"Synthetic Data Literature"},{"location":"overview/literature/#papers","text":"","title":"Papers"},{"location":"overview/literature/#sim-to-real-via-sim-to-sim-data-efficient-robotic-grasping-via-randomized-to-canonical-adaptation-networks","text":"Usecase Robotic Grasping Year 2019 Abstract Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to realworld ones. Using domain adaptation methods to cross this \u201creality gap\u201d requires a large amount of unlabelled realworld data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomizedto-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no realworld data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99%.","title":"Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks "},{"location":"overview/literature/#learning-from-simulated-and-unsupervised-images-through-adversarial-training","text":"Usecase Human Gaze Estimation Year 2017 Abstract With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator\u2019s output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a \u2018self-regularization\u2019 term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.","title":"Learning from Simulated and Unsupervised Images through Adversarial Training "},{"location":"overview/literature/#domain-randomization-for-transferring-deep-neural-networks-from-simulation-to-the-real-world","text":"Usecase Robotic Grasping Year 2017 Abstract Bridging the \u2018reality gap\u2019 that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.","title":"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World "},{"location":"overview/literature/#deep-drone-racing-from-simulation-to-reality-with-domain-randomization","text":"Usecase Drone Racing Year 2019 Abstract Dynamically changing environments, unreliable state estimation, and operation under severe resource constraints are fundamental challenges that limit the deployment of small autonomous drones. We address these challenges in the context of autonomous, vision-based drone racing in dynamic environments. A racing drone must traverse a track with possibly moving gates at high speed. We enable this functionality by combining the performance of a state-of-the-art planning and control system with the perceptual awareness of a convolutional neural network (CNN). The resulting modular system is both platform- and domain-independent: it is trained in simulation and deployed on a physical quadrotor without any fine-tuning. The abundance of simulated data, generated via domain randomization, makes our system robust to changes of illumination and gate appearance. To the best of our knowledge, our approach is the first to demonstrate zero-shot sim-to-real transfer on the task of agile drone flight. We extensively test the precision and robustness of our system, both in simulation and on a physical platform, and show significant improvements over the state of the art.","title":"Deep Drone Racing: From Simulation to Reality with Domain Randomization "},{"location":"overview/literature/#structured-domain-randomization-bridging-the-reality-gap-by-context-aware-synthetic-data","text":"Usecase Autonomous Vehicles Year 2020 Abstract We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDRgenerated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.","title":"Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data "},{"location":"overview/literature/#using-synthetic-data-to-train-neural-networks-is-model-based-reasoning","text":"Usecase ML Theory Year 2017 Abstract We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.","title":"Using Synthetic Data to Train Neural Networks is Model-Based Reasoning "},{"location":"overview/literature/#learning-from-synthetic-humans","text":"Usecase Human Pose Detection Year 2018 Abstract Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data.","title":"Learning from Synthetic Humans "},{"location":"overview/literature/#multi-modal-semantic-segmentation-using-synthetic-data","text":"Usecase Autonomous Vehicles Year 2019 Abstract Semantic understanding of scenes in threedimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true distance of separation to objects which are crucial for taking mission critical decisions. As a first step, in this work we investigate the possibility of semantically classifying different parts of a given scene in 3D by learning the underlying geometric context in addition to the texture cues BUT in the absence of labelled real-world datasets. To this end we generate a large number of synthetic scenes, their pixel-wise labels and corresponding 3D representations using CARLA software framework. We then build a deep neural network that learns underlying category specific 3D representation and texture cues from color information of the rendered synthetic scenes. Further on we apply the learned model on different real world datasets to evaluate its performance. Our preliminary investigation of results show that the neural network is able to learn the geometric context from synthetic scenes and effectively apply this knowledge to classify each point of a 3D representation of a scene in real-world.","title":"Multi Modal Semantic Segmentation using Synthetic Data "},{"location":"overview/literature/#semantic-understanding-of-foggy-scenes-with-purely-synthetic-data","text":"Usecase Autonomous Vehicles Year 2020 Abstract This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen realworld foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.","title":"Semantic Understanding of Foggy Scenes with Purely Synthetic Data "},{"location":"overview/literature/#synthetic-data-for-deep-learning","text":"Usecase Overview Year 2019 Abstract Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including syntheticto-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.","title":"Synthetic Data for Deep Learning "},{"location":"overview/literature/#learning-color-space-adaptation-from-synthetic-to-real-images-of-cirrus-clouds","text":"Usecase Cloud Detection Year 2020 Abstract Cloud segmentation plays a crucial role in image analysis for climate modeling. Manually labeling the training data for cloud segmentation is time-consuming and error-prone. We explore to train segmentation networks with synthetic data due to the natural acquisition of pixel-level labels. Nevertheless, the domain gap between synthetic and real images significantly degrades the performance of the trained model. We propose a color space adaptation method to bridge the gap, by training a color-sensitive generator and discriminator to adapt synthetic data to real images in color space. Instead of transforming images by general convolutional kernels, we adopt a set of closed-form operations to make color-space adjustments while preserving the labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and demonstrate the adaptation efficacy on the semantic segmentation task of cirrus clouds. With our adapted synthetic data for training the semantic segmentation, we achieve an improvement of 6:59% when applied to real images, superior to alternative methods.","title":"Learning color space adaptation from synthetic to real images of cirrus clouds "},{"location":"overview/literature/#unrealrox-an-improved-tool-for-acquiring-synthetic-data-from-virtual-3d-environments","text":"Usecase Framework Year 2021 Abstract Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a Python API for interacting with the virtual environment from Deep Learning frameworks.","title":"UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments "},{"location":"overview/literature/#the-synthia-dataset-a-large-collection-of-synthetic-images-for-semantic-segmentation-of-urban-scenes","text":"Usecase Autonomous Vehicles Year 2016 Abstract Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation \u2013 in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.","title":"The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes "},{"location":"overview/literature/#title","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/literature/#title_1","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/literature/#title_2","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/literature/#title_3","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/literature/#title_4","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/literature/#title_5","text":"Usecase Year Abstract","title":"Title "},{"location":"overview/what/","text":"Synthetic data is data that is created as opposed to collected . Synthetic data can be used in any of the flavors of Machine Learning (natural language, computer vision, tabular data).","title":"What is it?"},{"location":"overview/why/","text":"So why should you use synthetic data? Well, synthetic data is ... ... quicker to iterate. Synthetic data makes it easy to change the annotation style, or add an additional label which can be used as an additional training loss for the model. It also makes it easy to generate more examples of a specific edge case that may be causing issues in production. Synthetic data generation and iteration should be easy, and used in concert with adjustments to the model in order to achieve one\u2019s goals. ... virtually infinite. Modern machine learning requires larger and larger datasets with synthetic data you can scale a dataset to the required size to train high-performant models. ... perfectly labeled. At Zumo Labs, many of our incoming customers have a common pain point: labeled training data is presenting itself as a significant bottleneck. How is it that data wrangling (that is, sourcing labeled data and managing the training data pipeline) can take up to 80% of AI project time by some estimates. ... free of privacy risks and biases. Traditionally the problem has been that compiling useful data sets requires infringing on people\u2019s personal information, but guaranteeing privacy means either smaller or lower quality data sets, or stripping them of information to the point they are no longer useful.","title":"Why use it?"},{"location":"overview/why/#quicker-to-iterate","text":"Synthetic data makes it easy to change the annotation style, or add an additional label which can be used as an additional training loss for the model. It also makes it easy to generate more examples of a specific edge case that may be causing issues in production. Synthetic data generation and iteration should be easy, and used in concert with adjustments to the model in order to achieve one\u2019s goals.","title":"... quicker to iterate."},{"location":"overview/why/#virtually-infinite","text":"Modern machine learning requires larger and larger datasets with synthetic data you can scale a dataset to the required size to train high-performant models.","title":"... virtually infinite."},{"location":"overview/why/#perfectly-labeled","text":"At Zumo Labs, many of our incoming customers have a common pain point: labeled training data is presenting itself as a significant bottleneck. How is it that data wrangling (that is, sourcing labeled data and managing the training data pipeline) can take up to 80% of AI project time by some estimates.","title":"... perfectly labeled."},{"location":"overview/why/#free-of-privacy-risks-and-biases","text":"Traditionally the problem has been that compiling useful data sets requires infringing on people\u2019s personal information, but guaranteeing privacy means either smaller or lower quality data sets, or stripping them of information to the point they are no longer useful.","title":"... free of privacy risks and biases."},{"location":"releases/1.0-NOTES/","text":"First release of zpy! MVP for CLI MVP for Blender Addon","title":"v1.0"},{"location":"releases/1.1-NOTES/","text":"Features: Random material function Lighting Randomization function Fixes: Image and annotation filename changes Bugfixes and improvements to CLI and Addon Workflow: Documentation now lives on a dedicated site: https://zumolabs.github.io/zpy/ Removed sphynx documentation Code linting through GitHub Actions","title":"v1.1"},{"location":"releases/1.2-NOTES/","text":"Features: Added support for Blender 2.93. Added initial support for payment accounts and projects for better grouping of related objects. All zpy create calls will now require a project namespace to be set via zpy project set . Added more informative error messages for known API errors (HTTP code 400). Added more informative print of filtered data sets before the confirmation message when doing zpy create job . Fixes: Standardized column widths for zpy list calls to prevent clipping of UUIDs and dates.","title":"v1.2"},{"location":"releases/1.3-NOTES/","text":"Features cli now does zpy object action instead of zpy action object api rework around transforms, jobs, datasets API new API for ragnarok (zumolabs backend) collapse datasets to /api/v1/datasets add a generate call to /api/v1/datasets/<id>/generate datasets now have files as child objects /api/v1/datasets/<id>/files transforms added to convert between formats will now reused SimRuns on new generates where possible dataset names are unique across projects UI now focused on sims and datasets project now used for scoping","title":"v1.3"},{"location":"zpy/about/","text":"Abstract zpy is a python package that makes synthetic data easy, by simplifying the process of creating simulations, or sims . The zpy module contains multiple pieces: A Blender Addon for creating and debugging sims inside the Blender UI. A CLI for things like uploading sims and generating datasets. A WebApp for a GUI version of the API and CLI.","title":"About"},{"location":"zpy/about/#abstract","text":"zpy is a python package that makes synthetic data easy, by simplifying the process of creating simulations, or sims . The zpy module contains multiple pieces: A Blender Addon for creating and debugging sims inside the Blender UI. A CLI for things like uploading sims and generating datasets. A WebApp for a GUI version of the API and CLI.","title":"Abstract"},{"location":"zpy/example/package/","text":"Package The package sim spawns packages on a floor and takes images from a varying camera viewpoint. Boxes are individually segmented , and the resulting dataset is used for object detection. Results We trained a CNN on synthetic data produced by this sim. Below are images showing predictions from this network: Blog You can find the full blog post for this project here . Code The code for this example can be found here .","title":"Package Sim"},{"location":"zpy/example/package/#package","text":"The package sim spawns packages on a floor and takes images from a varying camera viewpoint. Boxes are individually segmented , and the resulting dataset is used for object detection.","title":"Package"},{"location":"zpy/example/package/#results","text":"We trained a CNN on synthetic data produced by this sim. Below are images showing predictions from this network:","title":"Results"},{"location":"zpy/example/package/#blog","text":"You can find the full blog post for this project here .","title":"Blog"},{"location":"zpy/example/package/#code","text":"The code for this example can be found here .","title":"Code"},{"location":"zpy/example/part1/","text":"Suzanne Code-Along: Part 1 Welcome to the first zpy tutorial! In this tutorial we will introduce the following concepts: Create a Blender sim Script the sim with python and zpy Generate synthetic data Code The code for this example can be found here Video You can watch this tutorial as a video on YouTube .","title":"Part 1"},{"location":"zpy/example/part1/#suzanne-code-along-part-1","text":"Welcome to the first zpy tutorial! In this tutorial we will introduce the following concepts: Create a Blender sim Script the sim with python and zpy Generate synthetic data","title":"Suzanne Code-Along: Part 1"},{"location":"zpy/example/part1/#code","text":"The code for this example can be found here","title":"Code"},{"location":"zpy/example/part1/#video","text":"You can watch this tutorial as a video on YouTube .","title":"Video"},{"location":"zpy/example/part2/","text":"Suzanne Code-Along: Part 2 If you are new we recommend Part 1 of this tutorial . In this tutorial we will introduce the following concepts: Logging Gin configuration Object jittering Depth images Code The code for this example can be found here Video You can watch this tutorial as a video on YouTube .","title":"Part 2"},{"location":"zpy/example/part2/#suzanne-code-along-part-2","text":"If you are new we recommend Part 1 of this tutorial . In this tutorial we will introduce the following concepts: Logging Gin configuration Object jittering Depth images","title":"Suzanne Code-Along: Part 2"},{"location":"zpy/example/part2/#code","text":"The code for this example can be found here","title":"Code"},{"location":"zpy/example/part2/#video","text":"You can watch this tutorial as a video on YouTube .","title":"Video"},{"location":"zpy/example/part3/","text":"Suzanne Code-Along: Part 3 If you are new we recommend Part 1 of this tutorial . In this tutorial we will introduce the following concepts: Material jittering HDRI Backgrounds HSV randomization Code The code for this example can be found here Video You can watch this tutorial as a video on YouTube .","title":"Part 3"},{"location":"zpy/example/part3/#suzanne-code-along-part-3","text":"If you are new we recommend Part 1 of this tutorial . In this tutorial we will introduce the following concepts: Material jittering HDRI Backgrounds HSV randomization","title":"Suzanne Code-Along: Part 3"},{"location":"zpy/example/part3/#code","text":"The code for this example can be found here","title":"Code"},{"location":"zpy/example/part3/#video","text":"You can watch this tutorial as a video on YouTube .","title":"Video"},{"location":"zpy/example/rpi/","text":"Raspberry Pi (rpi) The rpi sim takes images from a varying camera viewpoint of a raspberry pi board in the middle of the scene. Sub-components of the rpi are individually segmented , and the resulting dataset is used for object detection or segmentation. This sim makes use of Domain Randomization . Blog You can find the full blog post for this project here . Code The code for this example can be found here","title":"RPI Sim"},{"location":"zpy/example/rpi/#raspberry-pi-rpi","text":"The rpi sim takes images from a varying camera viewpoint of a raspberry pi board in the middle of the scene. Sub-components of the rpi are individually segmented , and the resulting dataset is used for object detection or segmentation. This sim makes use of Domain Randomization .","title":"Raspberry Pi (rpi)"},{"location":"zpy/example/rpi/#blog","text":"You can find the full blog post for this project here .","title":"Blog"},{"location":"zpy/example/rpi/#code","text":"The code for this example can be found here","title":"Code"},{"location":"zpy/install/blender_python_path/","text":"Blender's Python Path Blender comes with it's own Python, which is bundled with the 3D program. When installing python dependencies, such as 'zpy' a common mistake is to install them on the system python rather than Blender's python. More information on the install paths for Blender on various OS can be found in Blender's path documentation Linux If you are running Blender directly from the install folder, you can find the python and pip executables in the blender-2.93.0-linux64/2.93/python/bin/ folder. Windows On Windows, Blender's python path can be found in the program files directory C:\\Program Files\\Blender Foundation\\Blender 2.93\\2.93\\python\\bin","title":"Blender Python Path"},{"location":"zpy/install/blender_python_path/#blenders-python-path","text":"Blender comes with it's own Python, which is bundled with the 3D program. When installing python dependencies, such as 'zpy' a common mistake is to install them on the system python rather than Blender's python. More information on the install paths for Blender on various OS can be found in Blender's path documentation","title":"Blender's Python Path"},{"location":"zpy/install/blender_python_path/#linux","text":"If you are running Blender directly from the install folder, you can find the python and pip executables in the blender-2.93.0-linux64/2.93/python/bin/ folder.","title":"Linux"},{"location":"zpy/install/blender_python_path/#windows","text":"On Windows, Blender's python path can be found in the program files directory C:\\Program Files\\Blender Foundation\\Blender 2.93\\2.93\\python\\bin","title":"Windows"},{"location":"zpy/install/linux/","text":"Install Developer Environment on Linux First clone the zpy repository: mkdir -p $HOME/zumolabs && cd $HOME/zumolabs git clone https://github.com/ZumoLabs/zpy.git zpy Set the following environment variables: export ZPY_SRC_PATH=\"$HOME/zumolabs/zpy\" export BLENDER_VERSION=\"2.93\" export BLENDER_VERSION_FULL=\"2.93.0\" export BLENDER_PATH=\"$HOME/blender-${BLENDER_VERSION_FULL}-linux-x64/${BLENDER_VERSION}\" export BLENDER_LIB_PY=\"${BLENDER_PATH}/python/lib/python3.9\" export BLENDER_BIN_PY=\"${BLENDER_PATH}/python/bin/python3.9\" export BLENDER_BIN_PIP=\"${BLENDER_PATH}/python/bin/pip3\" NOTE The BLENDER_PATH variable might change depending on where you downloaded your Blender. Change the path accordingly. Install additional Python dependencies using Blender Python's pip: ${BLENDER_BIN_PY} -m ensurepip ${BLENDER_BIN_PIP} install --upgrade pip ${BLENDER_BIN_PIP} install -r ${ZPY_SRC_PATH}/requirements.txt If you are setting up a development environment it will be easier to symlink the zpy pip module directly into the Blender python library. This can be achieved with something like: ln -s ${ZPY_SRC_PATH}/zpy ${BLENDER_LIB_PY}/site-packages/ mkdir -p ~/.config/blender/${BLENDER_VERSION}/scripts/addons ln -s ${ZPY_SRC_PATH}/zpy_addon ~/.config/blender/${BLENDER_VERSION}/scripts/addons/zpy_addon","title":"Linux developer"},{"location":"zpy/install/linux/#install-developer-environment-on-linux","text":"First clone the zpy repository: mkdir -p $HOME/zumolabs && cd $HOME/zumolabs git clone https://github.com/ZumoLabs/zpy.git zpy Set the following environment variables: export ZPY_SRC_PATH=\"$HOME/zumolabs/zpy\" export BLENDER_VERSION=\"2.93\" export BLENDER_VERSION_FULL=\"2.93.0\" export BLENDER_PATH=\"$HOME/blender-${BLENDER_VERSION_FULL}-linux-x64/${BLENDER_VERSION}\" export BLENDER_LIB_PY=\"${BLENDER_PATH}/python/lib/python3.9\" export BLENDER_BIN_PY=\"${BLENDER_PATH}/python/bin/python3.9\" export BLENDER_BIN_PIP=\"${BLENDER_PATH}/python/bin/pip3\" NOTE The BLENDER_PATH variable might change depending on where you downloaded your Blender. Change the path accordingly. Install additional Python dependencies using Blender Python's pip: ${BLENDER_BIN_PY} -m ensurepip ${BLENDER_BIN_PIP} install --upgrade pip ${BLENDER_BIN_PIP} install -r ${ZPY_SRC_PATH}/requirements.txt If you are setting up a development environment it will be easier to symlink the zpy pip module directly into the Blender python library. This can be achieved with something like: ln -s ${ZPY_SRC_PATH}/zpy ${BLENDER_LIB_PY}/site-packages/ mkdir -p ~/.config/blender/${BLENDER_VERSION}/scripts/addons ln -s ${ZPY_SRC_PATH}/zpy_addon ~/.config/blender/${BLENDER_VERSION}/scripts/addons/zpy_addon","title":"Install Developer Environment on Linux"},{"location":"zpy/install/pip/","text":"Install using pip You can install zpy with pip: pip install zpy-zumo Note that Blender has it's own python , seperate from your system/venv/conda python. You will have to install it into both.","title":"pip"},{"location":"zpy/install/pip/#install-using-pip","text":"You can install zpy with pip: pip install zpy-zumo Note that Blender has it's own python , seperate from your system/venv/conda python. You will have to install it into both.","title":"Install using pip"},{"location":"zpy/install/script/","text":"Install using script We provide a install script for linux-subsystems (macos/linux). $ /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/ZumoLabs/zpy/main/install.sh)\" Set these environment variables for specific versions: export BLENDER_VERSION export BLENDER_VERSION_FULL export ZPY_VERSION","title":"script"},{"location":"zpy/install/script/#install-using-script","text":"We provide a install script for linux-subsystems (macos/linux). $ /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/ZumoLabs/zpy/main/install.sh)\" Set these environment variables for specific versions: export BLENDER_VERSION export BLENDER_VERSION_FULL export ZPY_VERSION","title":"Install using script"},{"location":"zpy/install/windows/","text":"Install Developer Environment on Windows These instructions use GitBash terminal, make sure to run as administrator! First clone the zpy repository: mkdir -p $HOME/zumolabs && cd $HOME/zumolabs git clone https://github.com/ZumoLabs/zpy.git zpy Set the following environment variables: export ZPY_SRC_PATH=\"$HOME/zumolabs/zpy\" export BLENDER_VERSION=\"2.93\" export BLENDER_PATH=\"/c/Program\\ Files/Blender\\ Foundation/Blender\\ ${BLENDER_VERSION}/${BLENDER_VERSION}\" export BLENDER_BIN_PY=\"${BLENDER_PATH}/python/bin/python.exe\" export BLENDER_BIN_PIP=\"${BLENDER_PATH}/python/bin/pip3\" If you are setting up a development environment it will be easier to symlink the zpy pip module directly into the Blender python library. This can be achieved with something like: ln -s ${ZPY_SRC_PATH}/zpy ${BLENDER_PATH}/python/lib/ ln -s ${ZPY_SRC_PATH}/zpy_addon ${BLENDER_PATH}/scripts/addons Install the dependencies ${BLENDER_BIN_PY} -m ensurepip ${BLENDER_BIN_PY} -m pip install --upgrade pip ${BLENDER_BIN_PY} -m pip install -r ${ZPY_SRC_PATH}/requirements.txt","title":"Windows developer"},{"location":"zpy/install/windows/#install-developer-environment-on-windows","text":"These instructions use GitBash terminal, make sure to run as administrator! First clone the zpy repository: mkdir -p $HOME/zumolabs && cd $HOME/zumolabs git clone https://github.com/ZumoLabs/zpy.git zpy Set the following environment variables: export ZPY_SRC_PATH=\"$HOME/zumolabs/zpy\" export BLENDER_VERSION=\"2.93\" export BLENDER_PATH=\"/c/Program\\ Files/Blender\\ Foundation/Blender\\ ${BLENDER_VERSION}/${BLENDER_VERSION}\" export BLENDER_BIN_PY=\"${BLENDER_PATH}/python/bin/python.exe\" export BLENDER_BIN_PIP=\"${BLENDER_PATH}/python/bin/pip3\" If you are setting up a development environment it will be easier to symlink the zpy pip module directly into the Blender python library. This can be achieved with something like: ln -s ${ZPY_SRC_PATH}/zpy ${BLENDER_PATH}/python/lib/ ln -s ${ZPY_SRC_PATH}/zpy_addon ${BLENDER_PATH}/scripts/addons Install the dependencies ${BLENDER_BIN_PY} -m ensurepip ${BLENDER_BIN_PY} -m pip install --upgrade pip ${BLENDER_BIN_PY} -m pip install -r ${ZPY_SRC_PATH}/requirements.txt","title":"Install Developer Environment on Windows"},{"location":"zpy/tutorials/depth/","text":"Saving Depth Images Depth images are single-channel images that indicate how far each pixel is from the camera. Video You can watch this tutorial as a video on YouTube:","title":"Depth Images"},{"location":"zpy/tutorials/depth/#saving-depth-images","text":"Depth images are single-channel images that indicate how far each pixel is from the camera.","title":"Saving Depth Images"},{"location":"zpy/tutorials/depth/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/hdri/","text":"Random HDRI Backgrounds An HDRI is an asset which contains an image (360 sphere) along with lighting information. These are used as backgrounds in sims. Video You can watch this tutorial as a video on YouTube:","title":"Random HDRI Backgrounds"},{"location":"zpy/tutorials/hdri/#random-hdri-backgrounds","text":"An HDRI is an asset which contains an image (360 sphere) along with lighting information. These are used as backgrounds in sims.","title":"Random HDRI Backgrounds"},{"location":"zpy/tutorials/hdri/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/jittermat/","text":"Jittering Materials Materials are properties of 3D assets that determine their appearance when rendered. Jittering materials is the process of procedurally randomizing materials during runtime. Material jittering is a form of Domain Randomization . Video You can watch this tutorial as a video on YouTube:","title":"Jitering Materials"},{"location":"zpy/tutorials/jittermat/#jittering-materials","text":"Materials are properties of 3D assets that determine their appearance when rendered. Jittering materials is the process of procedurally randomizing materials during runtime. Material jittering is a form of Domain Randomization .","title":"Jittering Materials"},{"location":"zpy/tutorials/jittermat/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/jitterpose/","text":"Jittering Object Pose Object pose is the position and rotation of an object in 3D space. Jittering object pose is the process of procedurally randomizing the position of an object (or set of objects) during runtime. Pose jittering is a form of Domain Randomization . Video You can watch this tutorial as a video on YouTube:","title":"Jittering Object Pose"},{"location":"zpy/tutorials/jitterpose/#jittering-object-pose","text":"Object pose is the position and rotation of an object in 3D space. Jittering object pose is the process of procedurally randomizing the position of an object (or set of objects) during runtime. Pose jittering is a form of Domain Randomization .","title":"Jittering Object Pose"},{"location":"zpy/tutorials/jitterpose/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/run_a_sim/","text":"Run a Sim Running a simulation can be done inside the Blender UI by clicking the zpy_addon panel button or Blender's \"script run\" button. Video You can watch this tutorial as a video on YouTube:","title":"Running a Sim"},{"location":"zpy/tutorials/run_a_sim/#run-a-sim","text":"Running a simulation can be done inside the Blender UI by clicking the zpy_addon panel button or Blender's \"script run\" button.","title":"Run a Sim"},{"location":"zpy/tutorials/run_a_sim/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/script_writing_guide/","text":"Script Writing Guide This guide gives you pointers and tips for writing a zpy run script. It is not meant to completely cover every single instance, but these things should be similar or the same over all sims. Blender File Setup In terms of actually setting the file up, you don\u2019t need a strict Collection Hierarchy, because it\u2019s so much easier to write the code and then vary the structure of the *.blend file as you change things over a project's lifecycle. Put like objects with like objects, for example: lights can be named and put in a collection with other lights that need similar functions enacted upon them. Collections and objects should be named with clarity in mind. Anyone should be able to discern what is in the sim by looking at the named objects. Imports Imports are flexible and should be added as needed, in general every run script should have the following imports: import bpy import zpy import logging Logging Use the zpy logger object, which uses Python's logging module log = logging.getLogger('zpy') log.info('This is an info log') log.debug('This is a debug log') You can set the log levels as such: zpy.logging.set_log_levels('debug') Decorators These decorators on the run function have different purposes, @gin.configurable('run') allows you to configure run function kwargs with gin. @zpy.blender.save_and_revert will save and revert the *.blend file every time you run it, allowing for local debugging. Run Kwargs The keyword arguments (kwargs) for the run function will be exposed to the end user. Figure out which configs are most useful for your project so you can toggle and change values when generating in the cloud. An example of run kwargs: def run( random_floor_tex: bool = True, jitter_mesh: bool = False, jitter_scale: bool = False, jitter_material: bool = False, use_distractors: bool = True, ): Seed Set the random seed to have repeatably random behavior. zpy.blender.set_seed(seed=43) Saving data Saver objects allow us to store all the metadata and annotations for the data we\u2019re generating. You can create one with this call: saver = zpy.saver_image.ImageSaver( description='description', ) Creating Segmentation Colors Create a segmentation color for each category and segment any objects: category_A_segmentation_color = zpy.color.random_color(output_style='frgb') saver.add_category( name='category_A', color=category_A_segmentation_color, ) zpy.objects.segment('object_name', color=category_A_segmentation_color, as_category=True, as_single=True) Saving Pose Save and restore the position of objects in the sim before each step. zpy.objects.save_pose('Camera') for frame in zpy.blender.step(): zpy.objects.restore_pose('Camera') Lighting Lighting can be added to a ILLUMINATION collection, inside the World Set up. Save lighting as a list, or iterate over the lighting using the bpy.data.Lighting type, this means the actual lighting within the scene can be changed or refactored without changing the script. If at all possible, do not reference objects by name, add them to the list and then manipulate that way. lighting = [] for obj in zpy.objects.for_obj_in_collections([ bpy.data.collections['ILLUMINATION'] ]): lighting.append(obj) The Loop Use frame as the iterator variable # Setup things for frame in zpy.blender.step(): # Do things Objects Restore pose of objects if needed. Then jitter. Jitter the mesh (slighty tweaks the points of the mesh), jitter the scale, jitter the rotation, jitter the position of the object. Always use \u2018if\u2019 logic to be able to turn the jitter on and off in the kwargs. Example: Following the logic of trying to work on things in lists for flexibility rather than naming specific objects, putting objects that need similar operations performed on them into Collections is useful. If all the objects you\u2019re using are Distractors that need to be randomly distributed around a zone, put them in the same Collection. Another option provided by Blender is the ability to filter by name. This allows us to create new objects or copy existing objects and then as long as the appropriate string is in their name, they\u2019ll work without changing the script. The last way of referring to objects without using their name is with a Type. If we want to change all the lighting for example, we could use: for obj in bpy.data.light: to get all the lights. Materials Jitter materials. There are three main types of material jitter, moving the material around a little, picking an entire random texture and randomly changing the material properties of a shader. Again, use IF logic to be able to switch these effects on and off with a bool. Object Distribution and Spawning To place items in a space, we can use a spawning algorithm written in python, which will vary depending on the needs of the sim. Another way to get native spawning inside of Blender is to use Geometry Nodes. This workflow isn\u2019t perfect, but it can save some time trying to figure out our own spawning methods. Add a new Geometry Node System with a Point Distribute and Point Instance Node. Randomize the locations of objects or objects within a Collection by linking seed value to frame. Turn off the Render Flag of the Spawner. Use zpy.select to select Spawner. Use \u201cMake Instances Real\u201d to create separate instances, then make each object a single user. Move the object to a separate Collection to isolate them. Loop through spawned objects to segment them as individual objects. Use if obj.name to apply category segments. Randomize the objects further as desired. Delete spawned instances of an object. A lot of these ending commands will be the same regardless of what sim you\u2019re working on, as they involve saving and output the images, which is similar between most sims. Rendering To render out images, first decide on names for the images. rgb_image_name = zpy.files.make_rgb_image_name(frame) iseg_image_name = zpy.files.make_iseg_image_name(frame) This call will actually render the images to file. zpy.render.render( rgb_path=saver.output_dir / rgb_image_name, iseg_path=saver.output_dir / iseg_image_name, width=image_width, height=image_height, ) Then add images to saver: saver.add_image( name=rgb_image_name, style='default', output_path=saver.output_dir / rgb_image_name, frame=frame, width=image_width, height=image_height, ) saver.add_image( name=iseg_image_name, style='segmentation', output_path=saver.output_dir / iseg_image_name, frame=frame, width=image_width, height=image_height, ) Annotations After working to randomize the scene, we need to add annotations to the scene. We have found it\u2019s easier to use name to filter through objects, but type or collection is a valid way of adding annotations. Example of using name to add a bounding box annotation: if 'box.' in obj.name: saver.add_annotation( image=rgb_image_name, seg_image=iseg_image_name, seg_color=tuple(obj.seg.instance_color), category=obj.seg.category_name, ) Writing out annotations with a populated saver object: zpy.output_zumo.OutputZUMO(saver).output_annotations() zpy.output_coco.OutputCOCO(saver).output_annotations()","title":"Script Writing Guide"},{"location":"zpy/tutorials/script_writing_guide/#script-writing-guide","text":"This guide gives you pointers and tips for writing a zpy run script. It is not meant to completely cover every single instance, but these things should be similar or the same over all sims.","title":"Script Writing Guide"},{"location":"zpy/tutorials/script_writing_guide/#blender-file-setup","text":"In terms of actually setting the file up, you don\u2019t need a strict Collection Hierarchy, because it\u2019s so much easier to write the code and then vary the structure of the *.blend file as you change things over a project's lifecycle. Put like objects with like objects, for example: lights can be named and put in a collection with other lights that need similar functions enacted upon them. Collections and objects should be named with clarity in mind. Anyone should be able to discern what is in the sim by looking at the named objects.","title":"Blender File Setup"},{"location":"zpy/tutorials/script_writing_guide/#imports","text":"Imports are flexible and should be added as needed, in general every run script should have the following imports: import bpy import zpy import logging","title":"Imports"},{"location":"zpy/tutorials/script_writing_guide/#logging","text":"Use the zpy logger object, which uses Python's logging module log = logging.getLogger('zpy') log.info('This is an info log') log.debug('This is a debug log') You can set the log levels as such: zpy.logging.set_log_levels('debug')","title":"Logging"},{"location":"zpy/tutorials/script_writing_guide/#decorators","text":"These decorators on the run function have different purposes, @gin.configurable('run') allows you to configure run function kwargs with gin. @zpy.blender.save_and_revert will save and revert the *.blend file every time you run it, allowing for local debugging.","title":"Decorators"},{"location":"zpy/tutorials/script_writing_guide/#run-kwargs","text":"The keyword arguments (kwargs) for the run function will be exposed to the end user. Figure out which configs are most useful for your project so you can toggle and change values when generating in the cloud. An example of run kwargs: def run( random_floor_tex: bool = True, jitter_mesh: bool = False, jitter_scale: bool = False, jitter_material: bool = False, use_distractors: bool = True, ):","title":"Run Kwargs"},{"location":"zpy/tutorials/script_writing_guide/#seed","text":"Set the random seed to have repeatably random behavior. zpy.blender.set_seed(seed=43)","title":"Seed"},{"location":"zpy/tutorials/script_writing_guide/#saving-data","text":"Saver objects allow us to store all the metadata and annotations for the data we\u2019re generating. You can create one with this call: saver = zpy.saver_image.ImageSaver( description='description', )","title":"Saving data"},{"location":"zpy/tutorials/script_writing_guide/#creating-segmentation-colors","text":"Create a segmentation color for each category and segment any objects: category_A_segmentation_color = zpy.color.random_color(output_style='frgb') saver.add_category( name='category_A', color=category_A_segmentation_color, ) zpy.objects.segment('object_name', color=category_A_segmentation_color, as_category=True, as_single=True)","title":"Creating Segmentation Colors"},{"location":"zpy/tutorials/script_writing_guide/#saving-pose","text":"Save and restore the position of objects in the sim before each step. zpy.objects.save_pose('Camera') for frame in zpy.blender.step(): zpy.objects.restore_pose('Camera')","title":"Saving Pose"},{"location":"zpy/tutorials/script_writing_guide/#lighting","text":"Lighting can be added to a ILLUMINATION collection, inside the World Set up. Save lighting as a list, or iterate over the lighting using the bpy.data.Lighting type, this means the actual lighting within the scene can be changed or refactored without changing the script. If at all possible, do not reference objects by name, add them to the list and then manipulate that way. lighting = [] for obj in zpy.objects.for_obj_in_collections([ bpy.data.collections['ILLUMINATION'] ]): lighting.append(obj)","title":"Lighting"},{"location":"zpy/tutorials/script_writing_guide/#the-loop","text":"Use frame as the iterator variable # Setup things for frame in zpy.blender.step(): # Do things","title":"The Loop"},{"location":"zpy/tutorials/script_writing_guide/#objects","text":"Restore pose of objects if needed. Then jitter. Jitter the mesh (slighty tweaks the points of the mesh), jitter the scale, jitter the rotation, jitter the position of the object. Always use \u2018if\u2019 logic to be able to turn the jitter on and off in the kwargs. Example: Following the logic of trying to work on things in lists for flexibility rather than naming specific objects, putting objects that need similar operations performed on them into Collections is useful. If all the objects you\u2019re using are Distractors that need to be randomly distributed around a zone, put them in the same Collection. Another option provided by Blender is the ability to filter by name. This allows us to create new objects or copy existing objects and then as long as the appropriate string is in their name, they\u2019ll work without changing the script. The last way of referring to objects without using their name is with a Type. If we want to change all the lighting for example, we could use: for obj in bpy.data.light: to get all the lights.","title":"Objects"},{"location":"zpy/tutorials/script_writing_guide/#materials","text":"Jitter materials. There are three main types of material jitter, moving the material around a little, picking an entire random texture and randomly changing the material properties of a shader. Again, use IF logic to be able to switch these effects on and off with a bool.","title":"Materials"},{"location":"zpy/tutorials/script_writing_guide/#object-distribution-and-spawning","text":"To place items in a space, we can use a spawning algorithm written in python, which will vary depending on the needs of the sim. Another way to get native spawning inside of Blender is to use Geometry Nodes. This workflow isn\u2019t perfect, but it can save some time trying to figure out our own spawning methods. Add a new Geometry Node System with a Point Distribute and Point Instance Node. Randomize the locations of objects or objects within a Collection by linking seed value to frame. Turn off the Render Flag of the Spawner. Use zpy.select to select Spawner. Use \u201cMake Instances Real\u201d to create separate instances, then make each object a single user. Move the object to a separate Collection to isolate them. Loop through spawned objects to segment them as individual objects. Use if obj.name to apply category segments. Randomize the objects further as desired. Delete spawned instances of an object. A lot of these ending commands will be the same regardless of what sim you\u2019re working on, as they involve saving and output the images, which is similar between most sims.","title":"Object Distribution and Spawning"},{"location":"zpy/tutorials/script_writing_guide/#rendering","text":"To render out images, first decide on names for the images. rgb_image_name = zpy.files.make_rgb_image_name(frame) iseg_image_name = zpy.files.make_iseg_image_name(frame) This call will actually render the images to file. zpy.render.render( rgb_path=saver.output_dir / rgb_image_name, iseg_path=saver.output_dir / iseg_image_name, width=image_width, height=image_height, ) Then add images to saver: saver.add_image( name=rgb_image_name, style='default', output_path=saver.output_dir / rgb_image_name, frame=frame, width=image_width, height=image_height, ) saver.add_image( name=iseg_image_name, style='segmentation', output_path=saver.output_dir / iseg_image_name, frame=frame, width=image_width, height=image_height, )","title":"Rendering"},{"location":"zpy/tutorials/script_writing_guide/#annotations","text":"After working to randomize the scene, we need to add annotations to the scene. We have found it\u2019s easier to use name to filter through objects, but type or collection is a valid way of adding annotations. Example of using name to add a bounding box annotation: if 'box.' in obj.name: saver.add_annotation( image=rgb_image_name, seg_image=iseg_image_name, seg_color=tuple(obj.seg.instance_color), category=obj.seg.category_name, ) Writing out annotations with a populated saver object: zpy.output_zumo.OutputZUMO(saver).output_annotations() zpy.output_coco.OutputCOCO(saver).output_annotations()","title":"Annotations"},{"location":"zpy/tutorials/segmentation/","text":"Segmentation Images Segmentation images mask out different objects in the image such that neural networks can train on the image. Video You can watch this tutorial as a video on YouTube:","title":"Segmentation Images"},{"location":"zpy/tutorials/segmentation/#segmentation-images","text":"Segmentation images mask out different objects in the image such that neural networks can train on the image.","title":"Segmentation Images"},{"location":"zpy/tutorials/segmentation/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/template/","text":"Using Script Templates Script templates in Blender are an easy way to start writing your script without having to write boiler plate. Video You can watch this tutorial as a video on YouTube:","title":"Using Script Templates"},{"location":"zpy/tutorials/template/#using-script-templates","text":"Script templates in Blender are an easy way to start writing your script without having to write boiler plate.","title":"Using Script Templates"},{"location":"zpy/tutorials/template/#video","text":"You can watch this tutorial as a video on YouTube:","title":"Video"},{"location":"zpy/tutorials/what_is_a_sim/","text":"What is a Sim? A simulation, also known as \"sim\", is a scriptable 3D environment that is created using the zpy tools and Blender. A sim have the following principal components: run.py - The main script executed at runtime which creates and controls the 3D environment. config.gin - An optional configuration file which allows custom configuration at runtime. main.blend - The primary blenderfile which contains the run script. Template Sim We provide some template simulations as examples and for getting started quickly: Package : Detection and segmentation of packages/boxes RPI : Detection and segmentation of Raspberry Pi components Suzanne : Detection and segmentation of Suzanne monkey heads","title":"What is a Sim?"},{"location":"zpy/tutorials/what_is_a_sim/#what-is-a-sim","text":"A simulation, also known as \"sim\", is a scriptable 3D environment that is created using the zpy tools and Blender. A sim have the following principal components: run.py - The main script executed at runtime which creates and controls the 3D environment. config.gin - An optional configuration file which allows custom configuration at runtime. main.blend - The primary blenderfile which contains the run script.","title":"What is a Sim?"},{"location":"zpy/tutorials/what_is_a_sim/#template-sim","text":"We provide some template simulations as examples and for getting started quickly: Package : Detection and segmentation of packages/boxes RPI : Detection and segmentation of Raspberry Pi components Suzanne : Detection and segmentation of Suzanne monkey heads","title":"Template Sim"}]}